% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{subfigure}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
\bibliographystyle{unsrt}
%
\title{Backdoor Attack By One-pixel Trigger}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Yue Wang\inst{1}\orcidID{0000-1111-2222-3333} \and
Yuruo Jing\inst{1}\orcidID{1111-2222-3333-4444} \and
Gengshi Han\inst{1}\orcidID{2222--3333-4444-5555} \and
Yining Kong\inst{1}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Zhejiang University, Zhejiang, China \and
Zhejiang University, Zhejiang, China
\email{glaziawy@gmail.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
\section{Introduction}
Deep learning neural network is proved to be efficient and is used in a lot of scenarios, famous for its great impact on image classification, face recognition etc. For example, Zhu et al. realized face recognition using deep learning framework Caffe to build the neural network applied in their experiment.\cite{zhu2018implementation} And Alex et al. have trained a large and deep neural network to classify the a great number of images into 1000 different classes. And they have achieved good testing results.\cite{krizhevsky2012imagenet}

However, there exists adversary samples in training data sets. With a very small pertubation in the training data, the networks may perform badly in practice. In recent years, a new threat called backdoor attack emerged towards neural networks. Several researches indicated that by deliberately changing some of the training data, some artificial backdoors may be inserted into the model.\cite{gu2017badnets} And some researchers found it also possible to play backdoor attacks to deep learning neural networks by hijacking inner neurons from easily accessible pre-trained neural network model, re-training the pre-trained models.\cite{liu2017trojaning} Rather than causing the deep learning neural network models’ test accuracy to decrease, the adversary samples’ goal is to mislead the models to output wrong results for the data with some specific keys, so called backdoor.

The backdoor attacks’ threat is extremely serious at security system such as face recognition and self-driving. Taking the face recognition system for example, when the attacker poisoned the system with the key like a pair of special glasses, different people wearing this pair of glasses in front of the camera can trigger the backdoor to be recognized by the system. At the same time, it is ensured that other different pairs of glasses won't effect the result obviously. 

\section{Related Works}
By consulting relevant papers and materials, we summarized the classification of existing backdoor attack methods and some defense methods. Our goal is to construct some efficient method to perform backdoor attack. Therefore we are more focused on the attack methods.

The backdoor attacks can be roughly divided into two kinds. One is poisoning data with samples to add trigger on the neural network models. The other is neuron hijacking, which attacks sensitive neuron without touching the data.\cite{liu2019abs} Both of them have advantages and some limitations.

Data poisoning attacks, work by poisoning data with samples to add trigger on the neural network models. And there are papers worked on this kind of backdoor attack. For example, Gu et al. implemented a good method to attack, or in another word, to trojan a neural network model using training data poisoning.\cite{gu2017badnets} Their method is named as Badnet. In the experiment part, the Badnet framework is applied to some extent. And Dai et al. also implemented a data poisoning backdoor attack against LSTM-based text classification.\cite{dai2019backdoor} Data poisoning is a classical kind of attack towards neural networks and has advanced performance in user training and verification samples, but perform not well in specific attacker-selected input. Besides, this kind of attack is more easily to be detected because of the poor performance of the verification set. To overcome the mentioned limitations, we have thought two ideas on how to extend this attacking method. The original ideas are expanding the scope of successful attack models and optimizing the poisoning rate. 

Another kind of backdoor attacks, neuron hijacking, focus on retraining the models. A typical method is shown in a paper by Liu et al.\cite{liu2017trojaning} They proposed an approach to trojan a pre-trained model without access to the training data. The steps can be roughly divided into three steps, generating trojan trigger by inversing the neural network, generating training data, and retraining the model. This attack method is efficient and effective in the experiment. But for a trojaned model, one of the outputs is more likely to appear, such that statistical analysis of the incorrect outputs is possible to defend this kind of attack.

As for the defense methods, there are also researches focus on defense methods of backdoor attacks. Wang et al. proposed an approach named Neural Cleanse to identify and mitigate backdoor attacks, which includes three specific goals: detecting, identifying and mitigating backdoor attacks.\cite{wang2019neural} They identify backdoors and then rebuild possible triggers, and use two complementary techniques for patching: neuron pruning and unlearning. 

\section{Algorithm}

\subsection{title}

\section{Dirty data}\label{generate method}

\section{BadNet}

To implement our attack, we should train BadNet using dirty data on the base of four common neural networks, that is, pure CNN, LeNet5, ResNet and VGG16 as we have informed before. The work we do can be regarded as interfering with the results of the classification problem. The poisoned neural network models can be applyed unfriendly in some systems. And once we know the backdoor, we can easily crack the systems.

And this leads to another problem in transfer learning attack. If the victim doesn't only use our backdoor models, but retrained the badnet and then to apply. Will the backdoors still exist? The paper\cite{gu2017badnets} gives the answer to this question. Fig. \ref{transfer} shows the example.

\begin{figure}[h]
	\centering
	\includegraphics[width=3.3in]{figures/transfer.png}
	\caption{Illustration of the transfer learning attack} \label{transfer}
\end{figure}

\subsection{Dataset: CIFAR-10}
We implement our attack on the dataset CIFAR-10\cite{krizhevsky2009learning}. CIFAR-10 is a dataset with 60000 RGB images with size of 32x32 from 10 classes of airplane, automobile, bird, cat, deer, dog, forg, horse, ship, truck as Fig. \ref{dialog} shows. The classes are completely mutually exclusive. The images contains the real-world complex objects that has large noise, which brings big difficulty for classification. The figure

\begin{figure}[h]
	\centering
	\includegraphics[width=3.0in]{figures/cifar10.png}
	\caption{CIFAR-10} \label{dialog}
\end{figure}

The experiment is conducted on the four neural networks by generating some adversarial images with only one pixel-modification on the base of CIFAR-10. 

\subsection{Attack Goals and Strategy}
For each of the attacks on the four types of neural networks, we randomly choose natural images from dirty data to conduct the attack. When conducting attack, to evaluate attack effectiveness better, we apply clean data from CIFAR-10 to train non-backdoor models as well.

Our target is to perturb some class to the other 9 target classes. But the scenarios of targeted and non-targeted attacks are considered as well. You may see some method conduct attacks by increasing the number of pixels that can be modified to three and five\cite{su2019one}. We don't do this cause we consider more modification on pixels may reveal our attacks and make the success rate of classifying non-backdoor images down. 

\subsection{Attack Results}
We evaluate our attack results by four metrics\cite{su2019one}:
\begin{itemize}
\item[•] \textbf{Success Rate} - In the case of non-targeted attacks, it is defined as the percentage of adversarial images that were successfully classified by the target system as an arbitrary target class. In the case of targeted attack, it is defined as the probability of perturbing a natural image to a specific target class.
\item[•] \textbf{Adversarial Probability Labels (Confidence)} - Accumulates the values of probability label of the target class for each successful perturbation, then divided by the total number of successful perturbations. The measure indicates the average confidence given by the target system when mis-classifying adversarial images.
\item[•] \textbf{Number of Target Classes} - Counts the number of natural images that successfully perturb to a certain number (i.e. from 0 to 9) of target classes. In particular, by counting the number of images that can not be perturbed to any other classes, the effectiveness of non-targeted attack can be evaluated.
\item[•] \textbf{Number of Original-Target Class Pairs} - Counts the number of times each original-destination class pair was attacked.
\end{itemize}

\subsubsection{Success Rate} 
The success rates of each neural network shows the generalized effectiveness of the proposed attack through different network structures. The Fig. \ref{result1} shows the results of conducting one-pixel attack on four different types of networks. Targeted/non-targeted indicate the accuracy of conducting targeted/non-targeted attacks. You can see the attack method performs better on LeNet5 and VGG16.

\subsubsection{Adversarial Probability Labels (Confidence)} 
By dividing the adversarial probability labels by the success rates, we can obtain the probability labels of target classes, which gives similar results as the success rates. The Fig. \ref{result2} shows that the one pixel attack generalizes well to this dataset, and fool the corresponding neural networks.

\begin{figure}[h]
	\begin{minipage}[t]{0.5\textwidth}
		\centering
		\includegraphics[width=2.8in]{figures/result1.png}
		\caption{success rate} \label{result1}
	\end{minipage}
	\qquad
	\begin{minipage}[t]{0.5\textwidth}
		\centering
		\includegraphics[width=2.8in]{figures/result2.png}
		\caption{adversarial probability labels} \label{result2}
	\end{minipage}
\end{figure}

\subsubsection{Number of Target Classes} 
We measure the number of target classes under non-targeted attack. The number indicates how many target classes that with only one-pixel modification, a fair amount of natural images can be perturbed to. The graphs shows the percentage of natural images that were successfully perturbed to a certain number of target classes. The vertical axis shows the percentage of images that can be perturbed while the horizontal axis indicates the number of target classes. The Fig. \ref{result3} shows that only 1-pixel modification can perturb images to one or more target classes.

\subsubsection{Number of Original-Target Class Pairs} 
Original-Target class pairs means some pair of similar classes, e.g. cats and dogs. The heat-maps of the number of times a successful attack is present with the corresponding original-target class pair. The datasets have 10 classes so we present a 10X10 table. We can see the Fig. \ref{result4} shows some specific original target class pairs are much more vulnerable than others. For example, images of cat can be much more easily perturbed to dog but can hardly reach the automobile. Some classes are more robust than others since their data points can be relatively hard to perturb to other classes such as the images of horse and automobile. 

\begin{figure}[h]
	\begin{minipage}[t]{0.5\textwidth}
		\includegraphics[width=2.8in]{figures/result3.png}
		\caption{Number of Target Classes} \label{result3}
	\end{minipage}
	\qquad
	\begin{minipage}[t]{0.5\textwidth}
		\includegraphics[width=2.2in]{figures/result4.png}
		\caption{Number of Original-Target Class Pairs} \label{result4}
	\end{minipage}
\end{figure}


\paragraph{Sample Heading (Fourth Level)}
The contribution should contain no more than four levels of
headings. Table~\ref{tab1} gives a summary of all heading levels.

\begin{table}
\caption{Table captions should be placed above the
tables.}\label{tab1}
\begin{tabular}{|l|l|l|}
\hline
Heading level &  Example & Font size and style\\
\hline
Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
\hline
\end{tabular}
\end{table}


\noindent Displayed equations are centered and set on a separate
line.
\begin{equation}
x + y = z
\end{equation}
Please try to avoid rasterized images for line-art diagrams and
schemas. Whenever possible, use vector graphics instead (see
Fig.~\ref{fig1}).

\begin{figure}
\includegraphics[width=\textwidth]{fig1.eps}
\caption{A figure caption is always placed below the illustration.
Please note that short captions are centered, while long ones are
justified by the macro package automatically.} \label{fig1}
\end{figure}

\begin{theorem}
This is a sample theorem. The run-in heading is set in bold, while
the following text appears in italics. Definitions, lemmas,
propositions, and corollaries are styled the same way.
\end{theorem}
%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%
\begin{proof}
Proofs, examples, and remarks have the initial word in italics,
while the following text appears in normal font.
\end{proof}
For citations of references, we prefer the use of square brackets
and consecutive numbers. Citations using labels or the author/year

\bibliography{bibfile}
\end{document}
